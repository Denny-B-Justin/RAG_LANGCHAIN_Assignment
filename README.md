# RAG_LANGCHAIN_Assignment

Task 1 – Creating Chatbot using RAG
Developed a RAG chatbot that can answer queries from PDF files. I used Gemini and Google API to develop the model. I loaded the data and stored them as data frame of page wise text data. Later, the text data is split into chunks for better storage and analysis. I specifically used chunk_size=1000 and chunk_overlap=20. Then the data is embedded using Google’s GenAI embeddings and stored as vectorstore.  Retrieval of data is done in vectorstore, I used Maximum marginal relevance (MMR) retrieval to balance between the relevance of documents and increase the model accuracy. I developed 3 template prompts and used them according to the need. Since the RAG model was more inclined towards accuracy and described data, I have used appropriate prompt template for them. The final chatbot is then developed.

Task 2 – Creating test dataset
To construct the test dataset, I had developed a specific prompt template for it. The prompt enabled me to create questions and answer the questions within itself. The prompt also converted the extracted data into data frame which I just had to save as CSV file. I am providing the prompt template and question below.
template4 = """You are an assistant for collecting data from the pdf. Make questions from the data and answer them, the main purpose is to create a dataset from the pdf. Do not include questions which does not have answer in the pdf file. Make sure your query-response pairs are diverse and are not concentrated on a specific query type, document section or page. Make questions such that 1 question per page. Create a dataframe as output and store them with a name. Always say "Thanks for using Athina AI!" at the end of the answer.
question = "Make a unique questions on 'Making a claim' from the PDF and name it as Questions to create a column, and answer them and store it in new column called Answers, now generate the dataframe named output_data. Make sure your query-response pairs are diverse and are not concentrated on a specific query type, document section or page. Make the questions starting with what,where, how, and who. I want to store it and use it later. write a code to make the dataframe."

Task 3 – Model Evaluation
To evaluate the model, I wanted to conduct four tests – two for retrieval and two for generator. Retriever evaluation is done using Context Precision and Context Recall matrices. Context precision metrics is used because it can measure how relevant the context of retrieval text with respect to question. Context recall matrices are used to measure the ability of the retriever to retrieve all important information needed to answer the questions. Both analysis requires retrieved context. On the other hand, Generator evaluation is done using faithfulness and Answer relevancy matrices. Faithfulness matrices is conducted by creating sentiments from generated answers followed by verifying each of the statements against the context. Faithfulness is critical in understanding the application complexity of the model. Answer relevancy helps to measure how relevant the answer is to the question. I chose these matrices because each matrix provides a specific perspective on the system's performance, helping to understand its strengths and weaknesses in different areas.
I tried to extract the report of the RAG model using giskard[llm]. But due to issues with my python version and transformers installation, I could not conduct evaluation of the model. I had got the output samples, but the reference and context data was missing to conduct the evaluation. I a looking for alternate methods for last couple of days, but due to lack of time, I have to submit the task without evaluation part.

•	Explain why you feel this is a comprehensive dataset to gauge the performance of your chatbot.

Ans: The dataset I have constructed have wide variety of questions asked from almost all corners of the pdf file enabling the document with relevant details. The dataset also requires context details and reference to undergo detailed evaluation of the model.

•	What did you try to improve the accuracy

Ans: The model accuracy can be increased in few methods. The chunk_size and chunk_overlap can be a good matrix to increase the accuracy of the model in overall. Choosing appropriate search_type for retrievers can  also increase the accuracy. I choose MMR search type for this model. Regularization techniques can also be applied to prevent overfitting, and cross-validation can provide a more reliable estimate of the model's performance.
